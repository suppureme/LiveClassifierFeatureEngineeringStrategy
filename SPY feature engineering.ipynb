{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:04.747048Z",
     "start_time": "2021-05-26T14:14:53.340719Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'watermark'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-5a116c01d22e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'load_ext'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'watermark'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'watermark'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'load_ext'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'autoreload'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'autoreload'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'2'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AT-Quant/My Strategies/Classifier+FeatureEngineering/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mrun_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2346\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'local_ns'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_local_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstack_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2347\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuiltin_trap\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2348\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2349\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2350\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AT-Quant/My Strategies/Classifier+FeatureEngineering/venv/lib/python3.7/site-packages/decorator.py\u001B[0m in \u001B[0;36mfun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mkwsyntax\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcaller\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mextras\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m     \u001B[0mfun\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m     \u001B[0mfun\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AT-Quant/My Strategies/Classifier+FeatureEngineering/venv/lib/python3.7/site-packages/IPython/core/magic.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f, *a, **k)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;31m# but it's overkill for just that one bit of state.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mmagic_deco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m         \u001B[0mcall\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AT-Quant/My Strategies/Classifier+FeatureEngineering/venv/lib/python3.7/site-packages/IPython/core/magics/extension.py\u001B[0m in \u001B[0;36mload_ext\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mmodule_str\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mUsageError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Missing module name.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m         \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshell\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextension_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_extension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mres\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'already loaded'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AT-Quant/My Strategies/Classifier+FeatureEngineering/venv/lib/python3.7/site-packages/IPython/core/extensions.py\u001B[0m in \u001B[0;36mload_extension\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     78\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mmodule_str\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mprepended_to_syspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mipython_extension_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m                     \u001B[0mmod\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mmod\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__file__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mipython_extension_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001B[0;32m/usr/lib/python3.7/importlib/__init__.py\u001B[0m in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m             \u001B[0mlevel\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_bootstrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gcd_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'watermark'"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import standard libs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as debug\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_relative_project_dir(project_repo_name=None, partial=True):\n",
    "    \"\"\"helper fn to get local project directory\"\"\"\n",
    "    current_working_directory = Path.cwd()\n",
    "    cwd_parts = current_working_directory.parts\n",
    "    if partial:\n",
    "        while project_repo_name not in cwd_parts[-1]:\n",
    "            current_working_directory = current_working_directory.parent\n",
    "            cwd_parts = current_working_directory.parts\n",
    "    else:\n",
    "        while cwd_parts[-1] != project_repo_name:\n",
    "            current_working_directory = current_working_directory.parent\n",
    "            cwd_parts = current_working_directory.parts\n",
    "    return current_working_directory\n",
    "\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import numba as nb\n",
    "\n",
    "# import ffn\n",
    "import yfinance as yf\n",
    "import bottleneck as bk\n",
    "import mlxtend as mlx\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import numpy_ext as npx\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import shap\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# import util libs\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# THESE ARE VARIABLES FOR EASILY ACCESSING DIFFERENT\n",
    "# DIRECTORIES FOR ACCESSING AND SAVING DATA AND IMAGES\n",
    "# IF NECESSARY. CHANGE THEM TO MATCH YOUR DIRECTORY\n",
    "# STRUCTURE.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "REPO_NAME = \"blackarbs_algo_strategy_dev\" # CHANGE TO MATCH YOUR REPO NAME\n",
    "print(\"\\n\", REPO_NAME)\n",
    "project_dir = get_relative_project_dir(REPO_NAME)\n",
    "data_dir = project_dir / \"data\"\n",
    "external = data_dir / \"external\"\n",
    "processed = data_dir / \"processed\"\n",
    "viz = project_dir / \"viz\"\n",
    "\n",
    "\n",
    "def cprint(df: pd.DataFrame, nrows: int = None):\n",
    "    \"\"\"\n",
    "    Custom dataframe print function\n",
    "    \"\"\"\n",
    "    if not isinstance(df, (pd.DataFrame,)):\n",
    "        try:\n",
    "            df = df.to_frame()\n",
    "        except:\n",
    "            raise ValueError(\"object cannot be coerced to df\")\n",
    "\n",
    "    if not nrows:\n",
    "        nrows = 5\n",
    "    print(\"*\" * 79)\n",
    "    print(\"dataframe information\")\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"HEAD num rows: {nrows}\")\n",
    "    print(df.head(nrows))\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"TAIL num rows: {nrows}\")\n",
    "    print(df.tail(nrows))\n",
    "    print(\"-\" * 50)\n",
    "    print(df.info())\n",
    "    print(\"*\" * 79)\n",
    "    print()\n",
    "    return\n",
    "\n",
    "\n",
    "print()\n",
    "%watermark -v -m -p numpy,pandas,scipy,sklearn,mlfinlab,seaborn,matplotlib -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:04.953120Z",
     "start_time": "2021-05-26T14:15:04.750047Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "sns_params = {\n",
    "    \"xtick.major.size\": 2,\n",
    "    \"ytick.major.size\": 2,\n",
    "    \"font.size\": 12,\n",
    "    \"font.weight\": \"medium\",\n",
    "    \"figure.figsize\": (10, 7),\n",
    "    # \"font.family\": \"Ubuntu Mono\",\n",
    "}\n",
    "sns.set_theme(\n",
    "    context=\"talk\",\n",
    "    style=\"white\",\n",
    "    # palette=sns.color_palette(\"magma\"),\n",
    "    rc=sns_params,\n",
    ")\n",
    "savefig_kwds = dict(dpi=90, bbox_inches=\"tight\", frameon=True, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:00.624933Z",
     "start_time": "2021-03-29T23:50:52.574296Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_tradestation_data(fn: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fn).rename(str.lower, axis=\"columns\")\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[\"date\"] + \" \" + df[\"time\"], infer_datetime_format=True\n",
    "    )\n",
    "    df[\"volume\"] = df[\"up\"] + df[\"down\"]\n",
    "    df.drop([\"date\", \"time\"], axis=1, inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "spy = read_tradestation_data(external / \"tradestation\" / \"SPY.txt\")\n",
    "cprint(spy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:02.909188Z",
     "start_time": "2021-03-29T23:51:00.629921Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_consolidator(df, period):\n",
    "    aggregate = {\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "        \"up\": \"sum\",\n",
    "        \"down\": \"sum\",\n",
    "        \"volume\": \"sum\",\n",
    "    }\n",
    "    return df.resample(f\"{period}Min\").agg(aggregate).dropna()\n",
    "\n",
    "\n",
    "data = dict()\n",
    "df = spy.copy()\n",
    "bars_5m = time_consolidator(df, 5)\n",
    "bars_30m = time_consolidator(df, 30)\n",
    "bars_1H = time_consolidator(df, 60)\n",
    "bars_1D = time_consolidator(df, 60 * 24)\n",
    "bars_1W = time_consolidator(df, 60 * 24 * 7)\n",
    "\n",
    "data = {\n",
    "    \"5m\": bars_5m,\n",
    "    \"30m\": bars_30m,\n",
    "    \"1H\": bars_1H,\n",
    "    \"1D\": bars_1D,\n",
    "    \"1W\": bars_1W,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Functions and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:03.745614Z",
     "start_time": "2021-03-29T23:51:02.913178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "\n",
    "\n",
    "def add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[f\"log_{column}_return\"] = np.log(df[column]).diff()\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def internal_bar_strength(df: pd.DataFrame) -> float:\n",
    "    return (df.close - df.low) / (df.high - df.low)\n",
    "\n",
    "\n",
    "def add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"ibs\"] = internal_bar_strength(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "# @nb.jit\n",
    "def aqr_momentum(array: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Input:  Price time series.\n",
    "    Output: Annualized exponential regression slope, \n",
    "            multiplied by the R2\n",
    "    \"\"\"    \n",
    "    returns = np.diff(np.log(array))  # .diff()\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def aqr_momo_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    r2 = 1 - resid / (y.size * y.var())\n",
    "    return (((1 + model[0]) ** 252) * r2)[0]\n",
    "\n",
    "\n",
    "def add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momentum, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momo_numba, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def get_slope(array: np.ndarray) -> float:\n",
    "    returns = np.diff(np.log(array))\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return slope\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def get_slope_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    # y = y[~np.isnan(y)]\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    return model[0]\n",
    "\n",
    "\n",
    "def add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope_numba, window, df[column].values, n_jobs=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_average_price(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"average_price\"] = (df.high + df.low + df.close + df.open) / 4\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmin_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmax_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# for some reason njit is generating zerodivision errors whereas numpy is not\n",
    "@nb.njit\n",
    "def numba_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in np.arange(len_df - window):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def numpy_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in tqdm(np.arange(len_df - window)):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    v = df.volume.values\n",
    "    avg = df[column].values\n",
    "    idx = df.index.asi8\n",
    "    # A = numba_vwap(avg, v, idx, len(df), window)\n",
    "    A = numpy_vwap(avg, v, idx, len(df), window)\n",
    "    outdf = (\n",
    "        pd.DataFrame(A, columns=[\"index\", f\"rvwap_{window}\"])\n",
    "        .assign(datetime=lambda df: pd.to_datetime(df[\"index\"], unit=\"ns\"))\n",
    "        .drop(\"index\", axis=1)\n",
    "        .set_index(\"datetime\")\n",
    "    )\n",
    "    df = df.join(outdf, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_bands(\n",
    "    df: pd.DataFrame, column: str, dist: int, window: int\n",
    ") -> pd.DataFrame:\n",
    "    upper = df[column] + dist * df[column].rolling(window).std()\n",
    "    lower = df[column] - dist * df[column].rolling(window).std()\n",
    "\n",
    "    df[f\"upper_band_{column}\"] = upper\n",
    "    df[f\"lower_band_{column}\"] = lower\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_acceleration(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    return_diff = df[column].pct_change().diff()\n",
    "    df[f\"racc_{column}_{window}\"] = return_diff.rolling(\n",
    "        window\n",
    "    ).std()  # standard deviation of second deriv aka acceleration\n",
    "    return df\n",
    "\n",
    "\n",
    "def roll_rank_bk(array):\n",
    "    rank = array.size + 1 - bk.rankdata(array)[-1]\n",
    "    A = array.shape[0]\n",
    "    p = rank / A\n",
    "    return p\n",
    "\n",
    "\n",
    "#################\n",
    "def add_volatility(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    returns = df[column].pct_change()\n",
    "    df[f\"rvol_{column}_{window}\"] = returns.rolling(window).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "def relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index(RSI) for given data.\n",
    "    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n:\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, \"high\"] - df.loc[i, \"high\"]\n",
    "        DoMove = df.loc[i, \"low\"] - df.loc[i + 1, \"low\"]\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\"RSI_\" + str(n))\n",
    "    return RSI\n",
    "\n",
    "\n",
    "def add_rsi(df: pd.DataFrame, column: str = \"close\", window: int = 14) -> pd.DataFrame:\n",
    "    out = df.reset_index()\n",
    "    rsi = relative_strength_index(out, window)\n",
    "    df[f\"rsi_{column}_{window}\"] = pd.Series(data=rsi.values, index=df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    rolling autocorrelation\n",
    "    \"\"\"\n",
    "    return npx.rolling_apply(\n",
    "        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\n",
    "        window,\n",
    "        array,\n",
    "        lag=lag,\n",
    "        n_jobs=10,\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rolling_autocorr(\n",
    "    df: pd.DataFrame, column: str, window: int, lag: int\n",
    ") -> pd.DataFrame:\n",
    "    log_changes_array = np.log(df[column]).diff().values\n",
    "    df[f\"racorr_{column}_{window}\"] = np_racorr(log_changes_array, window, lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "@nb.njit\n",
    "def custom_percentile(array: np.ndarray) -> float:\n",
    "    if (array.shape[0] - 1) == 0:\n",
    "        return np.nan\n",
    "    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\n",
    "\n",
    "\n",
    "def add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"rank_{column}_{window}\"] = npx.rolling_apply(\n",
    "        custom_percentile, window, df[column].values, n_jobs=5\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data store for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.242285Z",
     "start_time": "2021-03-29T23:51:03.749602Z"
    }
   },
   "outputs": [],
   "source": [
    "HDF_FILEPATH = processed / 'spy_features.h5'\n",
    "\n",
    "store = pd.HDFStore(\n",
    "    HDF_FILEPATH, mode=\"a\", complevel=1, complib=\"blosc:lz4\"\n",
    ")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.537852Z",
     "start_time": "2021-03-29T23:51:04.246275Z"
    }
   },
   "source": [
    "### add and save features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:13.771521Z",
     "start_time": "2021-05-26T14:15:13.548306Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_and_save_features(data, data_frequency, periods, hdf_filepath, rank_window=10):\n",
    "    \"\"\"\n",
    "    function to create features according to frequency and periods and save to\n",
    "    hdf store. store must already be created.\n",
    "\n",
    "    # Args\n",
    "        data: dict, keys are frequency, values are dataframes\n",
    "        data_frequency: str, one of the data frequencies from the data dict\n",
    "        periods: dict, keys are period labels, values are the integers\n",
    "        hdf_filepath: pathlib or str object\n",
    "        rank_window: int to divide window by for ranking features\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    log_errors = []\n",
    "    pprint(periods)\n",
    "\n",
    "    df = data[data_frequency].copy()\n",
    "\n",
    "    for key, window in tqdm(periods.items()):\n",
    "        tqdm._instances.clear()\n",
    "        try:\n",
    "            tmp_df = (\n",
    "                df.pipe(add_average_price)\n",
    "                .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "                .pipe(\n",
    "                    add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window\n",
    "                )\n",
    "                .pipe(add_internal_bar_strength)\n",
    "                .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "                .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "                .dropna()\n",
    "                .pipe(\n",
    "                    add_slope_column_numba,\n",
    "                    column=f\"lower_band_rvwap_{window}\",\n",
    "                    window=window,\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba,\n",
    "                    column=f\"upper_band_rvwap_{window}\",\n",
    "                    window=window,\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba, column=f\"rmin_low_{window}\", window=window\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba, column=f\"rmax_high_{window}\", window=window\n",
    "                )\n",
    "                .pipe(add_acceleration, column=\"close\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "                .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "                .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "                .pipe(add_acceleration, column=\"close\", window=window)\n",
    "                .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "                .pipe(add_volatility, column=\"close\", window=window)\n",
    "                .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "                .pipe(add_rsi, column=\"close\", window=window)\n",
    "                .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "                .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "                .pipe(\n",
    "                    add_rolling_autocorr, column=\"average_price\", window=window, lag=1\n",
    "                )\n",
    "            )\n",
    "            columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "            for column in tqdm(columns_to_rank):\n",
    "                tmp_df = add_custom_percentile(\n",
    "                    tmp_df, column, window=max(int(window // rank_window), rank_window)\n",
    "                )\n",
    "                # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "            # write to store iteratively in case of problems\n",
    "            with pd.HDFStore(hdf_filepath) as store:\n",
    "                store.put(\n",
    "                    value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\"\n",
    "                )\n",
    "\n",
    "        except Exception as error:\n",
    "            log_error = dict(window=window, error=error)\n",
    "            log_errors.append(log_error)\n",
    "    pprint(f\"{data_frequency} errors:\\n{log_errors}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T13:37:09.105474Z",
     "start_time": "2021-05-26T13:37:07.038144Z"
    }
   },
   "outputs": [],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 5)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "\n",
    "FREQ = '5m'\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:42:32.781498Z",
     "start_time": "2021-03-30T00:42:32.553109Z"
    }
   },
   "outputs": [],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 30)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "\n",
    "FREQ = \"30m\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hour (60 min) multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:59:36.628731Z",
     "start_time": "2021-03-30T00:52:17.331466Z"
    }
   },
   "outputs": [],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 60)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\", \"63_day\"]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1H\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:03:19.895463Z",
     "start_time": "2021-03-30T00:59:36.630727Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_day = 1\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [  # \"1_day\",\n",
    "    \"2_day\",\n",
    "    \"3_day\",\n",
    "    \"5_day\",\n",
    "    \"10_day\",\n",
    "    \"21_day\",\n",
    "    \"63_day\",\n",
    "    \"126_day\",\n",
    "    \"252_day\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [  # one_day,\n",
    "            two_days,\n",
    "            three_days,\n",
    "            five_days,\n",
    "            ten_days,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1D\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weekly features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:05:39.432425Z",
     "start_time": "2021-03-30T01:03:19.897408Z"
    }
   },
   "outputs": [],
   "source": [
    "one_week = 1\n",
    "two_weeks = int(one_week * 2)\n",
    "three_weeks = int(one_week * 3)\n",
    "one_month = int(one_week * 4)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [\n",
    "    # \"1_week\",\n",
    "    \"2_week\",\n",
    "    \"3_week\",\n",
    "    \"1_month\",\n",
    "    \"3_month\",\n",
    "    \"6_month\",\n",
    "    \"12_month\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [\n",
    "            # one_week,\n",
    "            two_weeks,\n",
    "            three_weeks,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1W\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}